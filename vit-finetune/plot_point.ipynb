{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the .pt files and extract statistics\n",
    "Assuming you have a set of .pt files with trained models, you need to load these files and extract the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_model_stats(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return trainable_params, total_params\n",
    "\n",
    "model_paths = [\"path_to_model1.pt\", \"path_to_model2.pt\", ...]\n",
    "model_stats = [get_model_stats(path) for path in model_paths]\n",
    "\n",
    "# Example model stats\n",
    "# model_stats = [(10000, 50000), (15000, 60000), ...]  # (trainable_params, total_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Calculate statistics\n",
    "For each model, you need to calculate the percentage of trainable parameters and gather the FID scores and training times (assuming you have these in some data structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_scores = [80, 22, 18, 16, 17]  # Example FID scores\n",
    "training_times = [100, 50, 20, 10, 30]  # Example training times (smaller is better)\n",
    "\n",
    "percent_trainable_params = [trainable / total for trainable, total in model_stats]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plot the data\n",
    "Using Matplotlib and Seaborn, you can create the plot with different bubble sizes for training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "methods = [\"LoRA-R8\", \"LoRA-R16\", \"VPT-Deep\", \"Adapt-Parallel\", \"Adapt-Sequential\", \"BitFit\", \"Full Fine-tuning\", \"DiffFit (Ours)\"]\n",
    "fid_scores = [80, 22, 18, 16, 17]\n",
    "training_times = [100, 50, 20, 10, 30]\n",
    "percent_trainable_params = [0.001, 0.002, 0.005, 0.01, 0.015]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(percent_trainable_params, fid_scores, s=[time*5 for time in training_times], alpha=0.5, edgecolor='w', label=methods)\n",
    "\n",
    "# Adding labels to each bubble\n",
    "for i, method in enumerate(methods):\n",
    "    plt.text(percent_trainable_params[i], fid_scores[i], method, fontsize=9, ha='right')\n",
    "\n",
    "plt.xlabel(\"Percentage of the Trainable Parameters\")\n",
    "plt.ylabel(\"Average Scores\")\n",
    "plt.title(\"Average score of fine-tuned ViT across 8 downstream datasets\")\n",
    "\n",
    "# Create a legend with bubble sizes\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
    "labels = [str(int(float(label)/5)) for label in labels]  # Convert back to original training times\n",
    "plt.legend(handles, labels, loc=\"upper right\", title=\"Training Time\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
