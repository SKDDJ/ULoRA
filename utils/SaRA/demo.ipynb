{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209bea0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pissa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from minsara import SaRAParametrization,add_sara, apply_to_sara, disable_sara, enable_sara, get_sara_params, merge_sara, name_is_sara, remove_sara,get_sara_state_dict\n",
    "_ = torch.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(150000)  # 举例增加到1500，根据实际需要调整\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492093a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2117, -0.2127, -0.4248]])\n"
     ]
    }
   ],
   "source": [
    "# a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=5, out_features=7),\n",
    "    torch.nn.Linear(in_features=7, out_features=3),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 5)\n",
    "y = model(x)\n",
    "print(y)\n",
    "Y0 = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ad9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "sara_config = {\n",
    "    nn.Linear: {\n",
    "        \"weight\": partial(SaRAParametrization.from_linear, rank=2),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98584a8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[2mSource path:... \u001b[22m/tmp/ipykernel_1762759/2490519804.py\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__name__ = '__main__'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__doc__ = 'Automatically created module for IPython interactive environment'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__package__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__loader__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__spec__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__builtin__ = <module 'builtins' (built-in)>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__builtins__ = <module 'builtins' (built-in)>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ih = ['', 'import torch\\nimport torch.nn as nn\\nfrom ...put_shape=(5,))\\n# assert torch.allclose(y, Y0)']\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_oh = {}\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_dh = [PosixPath('/root/shiym_proj/Sara/utils/SaRA'), PosixPath('/root/shiym_proj/Sara/utils/SaRA')]\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mIn = ['', 'import torch\\nimport torch.nn as nn\\nfrom ...put_shape=(5,))\\n# assert torch.allclose(y, Y0)']\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mOut = {}\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_ipython = <bound method InteractiveShell.get_ipython of <i...ll.ZMQInteractiveShell object at 0x7f1a343f9f30>>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mexit = <IPython.core.autocall.ZMQExitAutocall object at 0x7f1a343fabf0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mquit = <IPython.core.autocall.ZMQExitAutocall object at 0x7f1a343fabf0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mopen = <function open at 0x7f1a355b9ea0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ = <torch.autograd.grad_mode.set_grad_enabled object at 0x7f1a312aaad0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__ = ''\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m___ = ''\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__vsc_ipynb_file__ = '/root/shiym_proj/Sara/utils/SaRA/demo.ipynb'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i = 'sara_config = {\\n    nn.Linear: {\\n        \"wei...Parametrization.from_linear, rank=2),\\n    },\\n}'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ii = '# a simple model\\nmodel = torch.nn.Sequential(\\...orch.randn(1, 5)\\ny = model(x)\\nprint(y)\\nY0 = y'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_iii = 'import torch\\nimport torch.nn as nn\\nfrom funct....setrecursionlimit(150000)  # 举例增加到1500，根据实际需要调整'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i1 = 'import torch\\nimport torch.nn as nn\\nfrom funct....setrecursionlimit(150000)  # 举例增加到1500，根据实际需要调整'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mtorch = <module 'torch' from '/root/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/__init__.py'>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mnn = <module 'torch.nn' from '/root/anaconda3/envs/pi...b/python3.10/site-packages/torch/nn/__init__.py'>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mpartial = <class 'functools.partial'>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mSaRAParametrization = <class 'minsara.sara.SaRAParametrization'>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22madd_sara = <function add_sara at 0x7f18c0354790>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mapply_to_sara = <function apply_to_sara at 0x7f18c03545e0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mdisable_sara = <function <lambda> at 0x7f18c0355a20>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22menable_sara = <function <lambda> at 0x7f18c0354430>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_sara_params = <function get_sara_params at 0x7f18c0355c60>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mmerge_sara = <function merge_sara at 0x7f18c0354820>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mname_is_sara = <function name_is_sara at 0x7f18c0355ab0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mremove_sara = <function remove_sara at 0x7f18c0354670>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_sara_state_dict = <function get_sara_state_dict at 0x7f18c0355d80>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22msys = <module 'sys' (built-in)>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i2 = '# a simple model\\nmodel = torch.nn.Sequential(\\...orch.randn(1, 5)\\ny = model(x)\\nprint(y)\\nY0 = y'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mmodel = Sequential(  (0): Linear(in_features=5, out_feat...Linear(in_features=7, out_features=3, bias=True))\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mx = tensor([[ 1.5393,  0.6438, -0.3067,  0.1180, -0.8644]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22my = tensor([[-0.2117, -0.2127, -0.4248]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mY0 = tensor([[-0.2117, -0.2127, -0.4248]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i3 = 'sara_config = {\\n    nn.Linear: {\\n        \"wei...Parametrization.from_linear, rank=2),\\n    },\\n}'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22msara_config = {<class 'torch.nn.modules.linear.Linear'>: {'wei...s 'minsara.sara.SaRAParametrization'>>, rank=2)}}\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i4 = '# add sara to the model\\n# becase B is initiali...nput_shape=(5,))\\n# assert torch.allclose(y, Y0)'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mpysnooper = <module 'pysnooper' from '/root/anaconda3/envs/p.../python3.10/site-packages/pysnooper/__init__.py'>\u001b[0m\n",
      "\u001b[2m19:31:34.780701 line         5\u001b[0m SOURCE IS UNAVAILABLE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# add sara to the model\n",
    "# becase B is initialized to 0, the output is the same as before\n",
    "import pysnooper\n",
    "with pysnooper.snoop():\n",
    "    add_sara(model, sara_config=sara_config)\n",
    "y = model(x)\n",
    "\n",
    "# print(model)\n",
    "from labml.logger import inspect\n",
    "inspect(model)\n",
    "# from torchkeras import summary\n",
    "# summary(model, input_shape=(5,))\n",
    "# assert torch.allclose(y, Y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f06214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0251891",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5632, -0.2915, -0.0617]])\n"
     ]
    }
   ],
   "source": [
    "# to make the output different, we need to initialize B to something non-zero\n",
    "# model.apply(apply_to_sara(lambda x: torch.nn.init.ones_(x.lora_B)))\n",
    "y = model(x)\n",
    "print(y)\n",
    "assert not torch.allclose(y, Y0)\n",
    "Y1 = y\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196087bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now let's try to disable sara, the output is the same as before sara is added\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdisable_sara\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(y, Y0)\n",
      "File \u001b[0;32m~/shiym_proj/Sara/utils/SaRA/minsara/sara_utils.py:16\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_fn\n\u001b[1;32m     15\u001b[0m enable_sara \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: model\u001b[38;5;241m.\u001b[39mapply(apply_to_sara(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39menable_sara()))\n\u001b[0;32m---> 16\u001b[0m disable_sara \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_to_sara\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_sara\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# ------------------- helper function for collecting parameters for training/saving -------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_is_sara\u001b[39m(name):\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:890\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 890\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:890\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 890\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 890 (2965 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:890\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 890\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, fn: Callable[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m    Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    890\u001b[0m         module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m    891\u001b[0m     fn(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:2283\u001b[0m, in \u001b[0;36mModule.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \n\u001b[1;32m   2280\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m   2284\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m module\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:2301\u001b[0m, in \u001b[0;36mModule.named_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2287\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   2288\u001b[0m \n\u001b[1;32m   2289\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2298\u001b[0m \n\u001b[1;32m   2299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2300\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m-> 2301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2303\u001b[0m         memo\u001b[38;5;241m.\u001b[39madd(module)\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "# now let's try to disable sara, the output is the same as before sara is added\n",
    "disable_sara(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cba3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# enable sara again\n",
    "enable_sara(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f19300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the state dict for later use\n",
    "state_dict_to_save = get_sara_state_dict(model)\n",
    "state_dict_to_save.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a06b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can remove sara from the model\n",
    "remove_sara(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e71f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# lets try to load the sara back\n",
    "# first we need to add sara to the model\n",
    "add_sara(model)\n",
    "# then we can load the sara parameters\n",
    "# strict=False is needed because we are loading a subset of the parameters\n",
    "_ = model.load_state_dict(state_dict_to_save, strict=False) \n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c8570",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# we can merge it to make it a normal linear layer, so there is no overhead for inference\n",
    "merge_sara(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee283143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model now has no sara parameters\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3c246e1",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfaee1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=5, out_features=3)\n",
    "# Step 1: Add sara to the model\n",
    "add_sara(model)\n",
    "\n",
    "# Step 2: Collect the parameters, pass them to the optimizer\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": list(get_sara_params(model))},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "\n",
    "# Step 3: Train the model\n",
    "# ...\n",
    "# simulate training, update the sara parameters\n",
    "model.apply(apply_to_sara(lambda x: torch.nn.init.normal_(x.lora_A)))\n",
    "model.apply(apply_to_sara(lambda x: torch.nn.init.normal_(x.lora_B)))\n",
    "\n",
    "# Step 4: export the sara parameters\n",
    "state_dict = model.state_dict()\n",
    "sara_state_dict = {k: v for k, v in state_dict.items() if name_is_sara(k)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "539e7d19",
   "metadata": {},
   "source": [
    "## Loading and Inferencing with sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9836de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add sara to your model\n",
    "add_sara(model)\n",
    "\n",
    "# Step 2: Load the sara parameters\n",
    "_ = model.load_state_dict(sara_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Merge the sara parameters into the model\n",
    "merge_sara(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccba9d68",
   "metadata": {},
   "source": [
    "## Inferencing with multiple sara models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to avoid re-adding sara to the model when rerun the cell, remove sara first \n",
    "# remove_sara(model)\n",
    "# # Step 1: Add sara to your model\n",
    "# add_sara(model)\n",
    "\n",
    "# # Step 2: Load the sara parameters\n",
    "\n",
    "# # fake 3 sets of sara parameters\n",
    "# sara_state_dict_0 = sara_state_dict\n",
    "# sara_state_dict_1 = {k: torch.ones_like(v) for k, v in sara_state_dict.items()}\n",
    "# sara_state_dict_2 = {k: torch.zeros_like(v) for k, v in sara_state_dict.items()}\n",
    "# sara_state_dicts = [sara_state_dict_0, sara_state_dict_1, sara_state_dict_2]\n",
    "\n",
    "# load_multiple_sara(model, sara_state_dicts)\n",
    "\n",
    "# # Step 3: Select which sara to use at inference time\n",
    "# Y0 = select_sara(model, 0)(x)\n",
    "# Y1 = select_sara(model, 1)(x)\n",
    "# Y2 = select_sara(model, 2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67602a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y0, Y1, Y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_sara(model)\n",
    "init_state_dict = model.state_dict()\n",
    "# verify that it's the same as if we load the sara parameters one by one\n",
    "for state_dict in sara_state_dicts:\n",
    "    remove_sara(model)\n",
    "    _ = model.load_state_dict(init_state_dict, strict=False)\n",
    "    add_sara(model)\n",
    "    _ = model.load_state_dict(state_dict, strict=False)\n",
    "    merge_sara(model)\n",
    "    y = model(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Test():\n",
    "#     def __init__(self,num=1, layer=None):\n",
    "#         # self.layer = layer\n",
    "#         # for arg in args:\n",
    "#             # print(arg)\n",
    "#         self.layer = layer\n",
    "#         self.num = num\n",
    "# layer = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(5, 3),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(3, 3),\n",
    "#             torch.nn.ReLU()\n",
    "# )\n",
    "# test = Test(1,layer=layer)\n",
    "# # print(test)        \n",
    "# # print(test.num) # 1\n",
    "# # print(test.layer) # Linear(in_features=5, out_features=3, bias=True)\n",
    "# # inspect(test.layer)\n",
    "# print(test.layer[0])\n",
    "# print(test.layer[0].weight)\n",
    "# # print(test.layer[0]) \n",
    "# # print(test.layer[0].weight) \n",
    "# \"\"\"Parameter containing:\n",
    "# tensor([[-0.1163,  0.1544,  0.0566, -0.2275,  0.4066],\n",
    "#         [-0.0287, -0.3928,  0.2575, -0.1188, -0.0773],\n",
    "#         [-0.0870, -0.2780,  0.2427,  0.0463, -0.0287]], requires_grad=True)\"\"\"\n",
    "        \n",
    "# # print(test.layer.weight.shape) # torch.Size([3, 5])\n",
    "\n",
    "# # print(test.layer.weight.dtype) # torch.float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c847ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
