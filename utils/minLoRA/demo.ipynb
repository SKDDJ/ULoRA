{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209bea0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from minlora import add_lora, apply_to_lora, disable_lora, enable_lora, get_lora_params, merge_lora, name_is_lora, remove_lora, load_multiple_lora, select_lora, get_lora_state_dict\n",
    "_ = torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492093a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2740,  0.1809, -0.1765]])\n"
     ]
    }
   ],
   "source": [
    "# a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=5, out_features=7),\n",
    "    torch.nn.Linear(in_features=7, out_features=3),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 5)\n",
    "y = model(x)\n",
    "print(y)\n",
    "Y0 = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d9511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model.named_modules())\n",
    "# from labml.logger import inspect \n",
    "# # inspect(model.named_modules())\n",
    "# # model.named_modules()\n",
    "# for name, module in model.named_modules():\n",
    "#     # print(name) # 0 1\n",
    "#     # print(\"---\"*20)\n",
    "#     # print(name,module) \n",
    "#     \"\"\"Sequential(\n",
    "#   (0): Linear(in_features=5, out_features=7, bias=True)\n",
    "#   (1): Linear(in_features=7, out_features=3, bias=True)\n",
    "# )\n",
    "# Linear(in_features=5, out_features=7, bias=True)\n",
    "# Linear(in_features=7, out_features=3, bias=True)\"\"\"\n",
    "#     # inspect(module)\n",
    "#     if hasattr(module, \"parametrizations\"):\n",
    "#       print(\"yes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98584a8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ParametrizedLinear(\n",
      "    in_features=5, out_features=7, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ParametrizedLinear(\n",
      "    in_features=7, out_features=3, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[2mSource path:... \u001b[22m/tmp/ipykernel_1761884/1850508365.py\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__name__ = '__main__'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__doc__ = 'Automatically created module for IPython interactive environment'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__package__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__loader__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__spec__ = None\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__builtin__ = <module 'builtins' (built-in)>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__builtins__ = <module 'builtins' (built-in)>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ih = ['', 'import torch\\nfrom minlora import add_lora...)\\n\\nprint(model)\\nassert torch.allclose(y, Y0)']\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_oh = {}\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_dh = [PosixPath('/home/yimingshi/shiym_proj/Sarautils/minLoRA'), PosixPath('/home/yimingshi/shiym_proj/Sarautils/minLoRA')]\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mIn = ['', 'import torch\\nfrom minlora import add_lora...)\\n\\nprint(model)\\nassert torch.allclose(y, Y0)']\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mOut = {}\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_ipython = <bound method InteractiveShell.get_ipython of <i...ll.ZMQInteractiveShell object at 0x7fa9a9b5df60>>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mexit = <IPython.core.autocall.ZMQExitAutocall object at 0x7fa9a9b5ebc0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mquit = <IPython.core.autocall.ZMQExitAutocall object at 0x7fa9a9b5ebc0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mopen = <function open at 0x7fa9aad1dea0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ = <torch.autograd.grad_mode.set_grad_enabled object at 0x7fa9a81be650>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__ = ''\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m___ = ''\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m__vsc_ipynb_file__ = '/home/yimingshi/shiym_proj/Sarautils/minLoRA/demo.ipynb'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i = '# # print(model.named_modules())\\n# from labml....dule, \"parametrizations\"):\\n#       print(\"yes\")'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ii = '# a simple model\\nmodel = torch.nn.Sequential(\\...orch.randn(1, 5)\\ny = model(x)\\nprint(y)\\nY0 = y'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_iii = 'import torch\\nfrom minlora import add_lora, app...ra_state_dict\\n_ = torch.set_grad_enabled(False)'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i1 = 'import torch\\nfrom minlora import add_lora, app...ra_state_dict\\n_ = torch.set_grad_enabled(False)'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mtorch = <module 'torch' from '/root/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/__init__.py'>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22madd_lora = <function add_lora at 0x7fa837396200>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mapply_to_lora = <function apply_to_lora at 0x7fa8373965f0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mdisable_lora = <function <lambda> at 0x7fa837396710>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22menable_lora = <function <lambda> at 0x7fa837396680>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_lora_params = <function get_lora_params at 0x7fa837396950>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mmerge_lora = <function merge_lora at 0x7fa837396320>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mname_is_lora = <function name_is_lora at 0x7fa8373967a0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mremove_lora = <function remove_lora at 0x7fa8373963b0>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mload_multiple_lora = <function load_multiple_lora at 0x7fa837396c20>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mselect_lora = <function select_lora at 0x7fa837396d40>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mget_lora_state_dict = <function get_lora_state_dict at 0x7fa837396a70>\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i2 = '# a simple model\\nmodel = torch.nn.Sequential(\\...orch.randn(1, 5)\\ny = model(x)\\nprint(y)\\nY0 = y'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mmodel = Sequential(  (0): Linear(in_features=5, out_feat...Linear(in_features=7, out_features=3, bias=True))\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mx = tensor([[-0.7962,  0.3799,  0.0831,  0.5102,  0.4499]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22my = tensor([[ 0.2740,  0.1809, -0.1765]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mY0 = tensor([[ 0.2740,  0.1809, -0.1765]])\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i3 = '# # print(model.named_modules())\\n# from labml....dule, \"parametrizations\"):\\n#       print(\"yes\")'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22m_i4 = '# add lora to the model\\n# becase B is initiali...x)\\n\\nprint(model)\\nassert torch.allclose(y, Y0)'\u001b[0m\n",
      "\u001b[32m\u001b[2mNew var:....... \u001b[22mpysnooper = <module 'pysnooper' from '/root/anaconda3/envs/p.../python3.10/site-packages/pysnooper/__init__.py'>\u001b[0m\n",
      "\u001b[2m19:13:27.280233 line         5\u001b[0m SOURCE IS UNAVAILABLE\n",
      "\u001b[32m\u001b[2mModified var:.. \u001b[22mmodel = Sequential(  (0): ParametrizedLinear(    in_feat...       (0): LoRAParametrization()      )    )  ))\u001b[0m\n",
      "\u001b[2m19:13:27.286514 line         4\u001b[0m SOURCE IS UNAVAILABLE\n",
      "\u001b[33m\u001b[2mElapsed time: \u001b[22m00:00:00.008353\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# add lora to the model\n",
    "# becase B is initialized to 0, the output is the same as before\n",
    "import pysnooper\n",
    "with pysnooper.snoop():\n",
    "    add_lora(model)\n",
    "  \n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(model)\n",
    "assert torch.allclose(y, Y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0251891",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3777,  0.2858, -0.1262]])\n"
     ]
    }
   ],
   "source": [
    "# to make the output different, we need to initialize B to something non-zero\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.ones_(x.lora_B)))\n",
    "y = model(x)\n",
    "print(y)\n",
    "assert not torch.allclose(y, Y0)\n",
    "Y1 = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196087bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# now let's try to disable lora, the output is the same as before lora is added\n",
    "disable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9cba3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# enable lora again\n",
    "enable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f19300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0.parametrizations.weight.0.lora_A', '0.parametrizations.weight.0.lora_B', '1.parametrizations.weight.0.lora_A', '1.parametrizations.weight.0.lora_B'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's save the state dict for later use\n",
    "state_dict_to_save = get_lora_state_dict(model)\n",
    "state_dict_to_save.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a06b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can remove lora from the model\n",
    "remove_lora(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522e71f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# lets try to load the lora back\n",
    "# first we need to add lora to the model\n",
    "add_lora(model)\n",
    "# then we can load the lora parameters\n",
    "# strict=False is needed because we are loading a subset of the parameters\n",
    "_ = model.load_state_dict(state_dict_to_save, strict=False) \n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0c8570",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# we can merge it to make it a normal linear layer, so there is no overhead for inference\n",
    "merge_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee283143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=7, bias=True)\n",
       "  (1): Linear(in_features=7, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model now has no lora parameters\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3c246e1",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edfaee1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pissa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(in_features=5, out_features=3)\n",
    "# Step 1: Add LoRA to the model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Collect the parameters, pass them to the optimizer\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": list(get_lora_params(model))},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "\n",
    "# Step 3: Train the model\n",
    "# ...\n",
    "# simulate training, update the LoRA parameters\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_A)))\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_B)))\n",
    "\n",
    "# Step 4: export the LoRA parameters\n",
    "state_dict = model.state_dict()\n",
    "lora_state_dict = {k: v for k, v in state_dict.items() if name_is_lora(k)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "539e7d19",
   "metadata": {},
   "source": [
    "## Loading and Inferencing with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9836de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add LoRA to your model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Load the LoRA parameters\n",
    "_ = model.load_state_dict(lora_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Merge the LoRA parameters into the model\n",
    "merge_lora(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccba9d68",
   "metadata": {},
   "source": [
    "## Inferencing with multiple LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0ef4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid re-adding lora to the model when rerun the cell, remove lora first \n",
    "remove_lora(model)\n",
    "# Step 1: Add LoRA to your model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Load the LoRA parameters\n",
    "\n",
    "# fake 3 sets of LoRA parameters\n",
    "lora_state_dict_0 = lora_state_dict\n",
    "lora_state_dict_1 = {k: torch.ones_like(v) for k, v in lora_state_dict.items()}\n",
    "lora_state_dict_2 = {k: torch.zeros_like(v) for k, v in lora_state_dict.items()}\n",
    "lora_state_dicts = [lora_state_dict_0, lora_state_dict_1, lora_state_dict_2]\n",
    "\n",
    "load_multiple_lora(model, lora_state_dicts)\n",
    "\n",
    "# Step 3: Select which LoRA to use at inference time\n",
    "Y0 = select_lora(model, 0)(x)\n",
    "Y1 = select_lora(model, 1)(x)\n",
    "Y2 = select_lora(model, 2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c67602a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1677,  0.1617, -0.8168]]),\n",
       " tensor([[1.2984, 0.5447, 0.2660]]),\n",
       " tensor([[ 0.6715, -0.0822, -0.3609]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0, Y1, Y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537c5c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1677,  0.1617, -0.8168]])\n",
      "tensor([[1.2984, 0.5447, 0.2660]])\n",
      "tensor([[ 0.6715, -0.0822, -0.3609]])\n"
     ]
    }
   ],
   "source": [
    "remove_lora(model)\n",
    "init_state_dict = model.state_dict()\n",
    "# verify that it's the same as if we load the lora parameters one by one\n",
    "for state_dict in lora_state_dicts:\n",
    "    remove_lora(model)\n",
    "    _ = model.load_state_dict(init_state_dict, strict=False)\n",
    "    add_lora(model)\n",
    "    _ = model.load_state_dict(state_dict, strict=False)\n",
    "    merge_lora(model)\n",
    "    y = model(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eb3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Test():\n",
    "#     def __init__(self,num=1, layer=None):\n",
    "#         # self.layer = layer\n",
    "#         # for arg in args:\n",
    "#             # print(arg)\n",
    "#         self.layer = layer\n",
    "#         self.num = num\n",
    "# layer = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(5, 3),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(3, 3),\n",
    "#             torch.nn.ReLU()\n",
    "# )\n",
    "# test = Test(1,layer=layer)\n",
    "# # print(test)        \n",
    "# # print(test.num) # 1\n",
    "# # print(test.layer) # Linear(in_features=5, out_features=3, bias=True)\n",
    "# # inspect(test.layer)\n",
    "# print(test.layer[0])\n",
    "# print(test.layer[0].weight)\n",
    "# # print(test.layer[0]) \n",
    "# # print(test.layer[0].weight) \n",
    "# \"\"\"Parameter containing:\n",
    "# tensor([[-0.1163,  0.1544,  0.0566, -0.2275,  0.4066],\n",
    "#         [-0.0287, -0.3928,  0.2575, -0.1188, -0.0773],\n",
    "#         [-0.0870, -0.2780,  0.2427,  0.0463, -0.0287]], requires_grad=True)\"\"\"\n",
    "        \n",
    "# # print(test.layer.weight.shape) # torch.Size([3, 5])\n",
    "\n",
    "# # print(test.layer.weight.dtype) # torch.float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c847ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
