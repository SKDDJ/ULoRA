{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209bea0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pissa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from minsara import SaRAParametrization,add_sara, apply_to_sara, disable_sara, enable_sara, get_sara_params, merge_sara, name_is_sara, remove_sara,get_sara_state_dict\n",
    "# _ = torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870a58e",
   "metadata": {},
   "source": [
    "# a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492093a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 正确地将模型定义为类的属性\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=15, out_features=15),\n",
    "            # nn.ReLU(),  # 可选：添加一个非线性激活层以提升模型的表达能力\n",
    "            # nn.Linear(in_features=70, out_features=),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播\n",
    "        return self.model(x)\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     # 返回模型的简化字符串表示\n",
    "    #     return \"<MyModel with 2 layers>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ad9885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RANDOM x tensor([[ 0.0924, -0.8733,  0.7537, -1.7091,  2.2377, -0.9350, -0.3164,  0.7776,\n",
      "         -0.2826,  0.3074,  0.3646, -0.7074, -2.3209, -0.3096, -1.8750]])\n",
      "original y is tensor([[-1.6596,  0.8482,  0.1055,  0.6464, -0.0511,  0.8792,  0.7584,  1.3777,\n",
      "          0.5091, -0.3930, -0.0987, -0.7105,  0.0673, -1.3950,  0.4987]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "\n",
    "x = torch.randn(1, 15)\n",
    "print(\"The RANDOM x\",x)\n",
    "y = model(x)\n",
    "print(\"original y is\",y) # original y is tensor([[ 0.1539, -0.4083, -0.3811]])\n",
    "# Y0 = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98584a8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sara_config = {\n",
    "    nn.Linear: {\n",
    "        \"weight\": partial(SaRAParametrization.from_linear, rank=15),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4cb63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vector_parameters(model):\n",
    "    r\"\"\"\n",
    "    Returns the number of trainable parameters and number of all parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    vector_params = 0\n",
    "    all_param = 0\n",
    "    for n, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_param += num_params\n",
    "        if 'original_module' in n:\n",
    "            continue\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "            # if \"lora_\" not in n:\n",
    "            #     print(n)\n",
    "            if \"vector_z\" in n:\n",
    "                vector_params += num_params\n",
    "    print(\n",
    "        f\"vector params: {vector_params:,d} || trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    return vector_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d015869",
   "metadata": {},
   "source": [
    "# add sara to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f06214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector params: 15 || trainable params: 705 || all params: 705 || trainable%: 100.0\n",
      "vector params: 15 || trainable params: 15 || all params: 705 || trainable%: 2.127659574468085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_sara(model, sara_config=sara_config)\n",
    "# model.eval()\n",
    "# 遍历模型的所有参数\n",
    "# 冻结所有层的梯度\n",
    "print_vector_parameters(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 假设我们有一个特定的层 layer_to_train，\n",
    "# 我们想要为它启用梯度\n",
    "for param in get_sara_params(model):\n",
    "    param.requires_grad = True\n",
    "print_vector_parameters(model)\n",
    "    \n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name} : {param.requires_grad}\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name} 是可训练的: {param.requires_grad}\")\n",
    "        \n",
    "# parameters = [\n",
    "#     {\"params\": list(get_sara_params(model))},\n",
    "# ]\n",
    "# print(parameters)\n",
    "# exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0251891",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y after add sara tensor([[-1.6596,  0.8482,  0.1055,  0.6464, -0.0511,  0.8792,  0.7584,  1.3777,\n",
      "          0.5091, -0.3930, -0.0987, -0.7105,  0.0673, -1.3950,  0.4987]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "MyModel(\n",
      "  (model): Sequential(\n",
      "    (0): ParametrizedLinear(\n",
      "      in_features=15, out_features=15, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): SaRAParametrization()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "y = model(x)\n",
    "print(\"y after add sara\",y) # y after add lora tensor([[ 0.2840, -0.3440, -0.4243]])\n",
    "print(model)  # <MyModel with 2 layers>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c7495",
   "metadata": {},
   "source": [
    "## get the sara params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18dfb33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.parametrizations.weight.0.vector_z torch.Size([15])\n",
      "Parameter containing:\n",
      "tensor([2.2514, 1.9614, 1.6007, 1.5082, 1.3850, 1.2004, 1.1712, 0.9489, 0.7911,\n",
      "        0.7053, 0.6024, 0.4700, 0.3626, 0.2641, 0.0294], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "aaa = get_sara_params(model, print_shapes=True)\n",
    "\n",
    "for item in aaa:\n",
    "    # print the trainable params\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d3ac9",
   "metadata": {},
   "source": [
    "# now let's try to disable sara, the output is the same as before sara is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196087bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y after disable sara tensor([[-1.6596,  0.8482,  0.1055,  0.6464, -0.0511,  0.8792,  0.7584,  1.3777,\n",
      "          0.5091, -0.3930, -0.0987, -0.7105,  0.0673, -1.3950,  0.4987]])\n"
     ]
    }
   ],
   "source": [
    "# now let's try to disable sara, the output is the same as before sara is added\n",
    "disable_sara(model)\n",
    "y = model(x)\n",
    "# assert torch.allclose(y, Y0)\n",
    "print(\"y after disable sara\",y) #y after disable sara tensor([[ 0.1539, -0.4083, -0.3811]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef78e8",
   "metadata": {},
   "source": [
    "# enable sara again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9cba3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_sara again tensor([[-1.6596,  0.8482,  0.1055,  0.6464, -0.0511,  0.8792,  0.7584,  1.3777,\n",
      "          0.5091, -0.3930, -0.0987, -0.7105,  0.0673, -1.3950,  0.4987]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enable_sara(model)\n",
    "y = model(x)\n",
    "# assert torch.allclose(y, Y1)\n",
    "print(\"enable_sara again\",y) # enable_sara again tensor([[ 0.2840, -0.3440, -0.4243]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6c039",
   "metadata": {},
   "source": [
    "# let's save the state dict for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f19300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.0.parametrizations.weight.0.vector_z'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_dict_to_save = get_sara_state_dict(model)\n",
    "state_dict_to_save.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff0122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (model): Sequential(\n",
      "    (0): ParametrizedLinear(\n",
      "      in_features=15, out_features=15, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): SaRAParametrization()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4c97c",
   "metadata": {},
   "source": [
    "# you can remove sara from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a06b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_sara(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcd79a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb78dc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.0.parametrizations.weight.0.vector_z': tensor([2.2514, 1.9614, 1.6007, 1.5082, 1.3850, 1.2004, 1.1712, 0.9489, 0.7911,\n",
       "         0.7053, 0.6024, 0.4700, 0.3626, 0.2641, 0.0294])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_to_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a979b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.bias torch.Size([15])\n",
      "model.0.weight torch.Size([15, 15])\n"
     ]
    }
   ],
   "source": [
    "# # 假设 'model' 是您的 MyModel 实例\n",
    "# # Sequential 容器是通过 'model.model' 访问的\n",
    "# # ParametrizedLinear 层是 Sequential 容器的第一个模块\n",
    "\n",
    "# # 首先获取 Sequential 容器内的 ParametrizedLinear 实例\n",
    "# parametrized_linear_layer = model.model[0]\n",
    "\n",
    "# # 现在，parametrized_linear_layer 是 ParametrizedLinear 的一个实例\n",
    "# # 您可以直接从中访问 parametrizations 属性\n",
    "# # print(model)\n",
    "# # from labml.logger import inspect\n",
    "# # inspect(model)\n",
    "# # inspect(parametrized_linear_layer.parametrizations)\n",
    "\n",
    "# import pysnooper\n",
    "# with pysnooper.snoop():\n",
    "#     parametrization = parametrized_linear_layer.parametrizations['weight'][0]\n",
    "#     vector_z = parametrization.vector_z\n",
    "\n",
    "# print(\"first_submodule_parametrizations.weight[0].vector_z\", vector_z)\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed9d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Layer (type)                            Output Shape              Param #\n",
      "==========================================================================\n",
      "Linear-1                                    [-1, 15]                  240\n",
      "==========================================================================\n",
      "Total params: 240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 240\n",
      "--------------------------------------------------------------------------\n",
      "Input size (MB): 0.000057\n",
      "Forward/backward pass size (MB): 0.000114\n",
      "Params size (MB): 0.000916\n",
      "Estimated Total Size (MB): 0.001087\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------------------------------------\\nLayer (type)                            Output Shape              Param #\\n==========================================================================\\nLinear-1                                    [-1, 15]                  240\\n==========================================================================\\nTotal params: 240\\nTrainable params: 0\\nNon-trainable params: 240\\n--------------------------------------------------------------------------\\nInput size (MB): 0.000057\\nForward/backward pass size (MB): 0.000114\\nParams size (MB): 0.000916\\nEstimated Total Size (MB): 0.001087\\n--------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchkeras import summary\n",
    "summary(model, input_shape=(15,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "522e71f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (model): Sequential(\n",
      "    (0): ParametrizedLinear(\n",
      "      in_features=15, out_features=15, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): SaRAParametrization()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "model.0.bias torch.Size([15])\n",
      "model.0.parametrizations.weight.original torch.Size([15, 15])\n",
      "model.0.parametrizations.weight.0.lora_A torch.Size([15, 15])\n",
      "model.0.parametrizations.weight.0.lora_B torch.Size([15, 15])\n",
      "model.0.parametrizations.weight.0.vector_z torch.Size([15])\n",
      "model.0.parametrizations.weight.0.lora_dropout_mask torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "# lets try to load the sara back\n",
    "# first we need to add sara to the model\n",
    "add_sara(model, sara_config=sara_config)\n",
    "\n",
    "print(model)\n",
    "# then we can load the sara parameters\n",
    "# strict=False is needed because we are loading a subset of the parameters\n",
    "_ = model.load_state_dict(state_dict_to_save, strict=False) \n",
    "# y = model(x)\n",
    "# print(\"add sara again after remove sara\",y) # add sara again after remove sara tensor([[ 0.2840, -0.3440, -0.4243]])\n",
    "# assert torch.allclose(y, Y1)\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f0c8570",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.bias torch.Size([15])\n",
      "model.0.weight torch.Size([15, 15])\n"
     ]
    }
   ],
   "source": [
    "# we can merge it to make it a normal linear layer, so there is no overhead for inference\n",
    "merge_sara(model)\n",
    "# y = model(x)\n",
    "# print(\"after merge the sara\",y) # after merge the sara tensor([[ 0.2840, -0.3440, -0.4243]])\n",
    "# assert torch.allclose(y, Y1)\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab07194",
   "metadata": {},
   "source": [
    "# model now has no sara parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee283143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3c246e1",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edfaee1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "(torch.Size([30, 30]), 768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Step 1: Add sara to the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43madd_sara\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Collect the parameters, pass them to the optimizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_sara_params(model))},\n\u001b[1;32m      9\u001b[0m ]\n",
      "File \u001b[0;32m~/shiym_proj/Sara/utils/SaRA/minsara/sara.py:281\u001b[0m, in \u001b[0;36madd_sara\u001b[0;34m(model, sara_config)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"add sara parametrization to all layers in a model. Calling it twice will add sara twice\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#给模型中所有层添加SaRA参数化。如果调用两次，会添加两次SaRA。\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_sara\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msara_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msara_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:891\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    890\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 891\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/shiym_proj/Sara/utils/SaRA/minsara/sara.py:262\u001b[0m, in \u001b[0;36mapply_sara\u001b[0;34m(layer, register, merge, sara_config)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m attr_name, parametrization \u001b[38;5;129;01min\u001b[39;00m sara_config[\u001b[38;5;28mtype\u001b[39m(layer)]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;66;03m# if not hasattr(layer.parametrizations, attr_name):  # 检查是否已经存在参数化\u001b[39;00m\n\u001b[1;32m    256\u001b[0m                 \u001b[38;5;66;03m# print(f\"{attr_name} 没有参数化\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;66;03m# attr_name:\"weight\"; parametrization: partial(SaRAParametrization.from_linear, rank=8) \u001b[39;00m\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;66;03m# 一个`partial`对象，包含了一个参数化函数和一些参数。\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"`register_parametrization` 是用来在 PyTorch 框架中对特定模块的张量（通常是一个网络层的权重或偏置）注册一个参数化操作。实际上，参数化指的是对张量应用一个可训练的变换，通常用于实现复杂的正则化、约束或改良模型架构的目的。\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m                 \u001b[43mparametrize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_parametrization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparametrization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;66;03m#    对每个属性和参数化，调用`register_parametrization`来在层上注册SaRA参数化。\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m             \n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# this will remove all parametrizations, use with caution\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m#    如果`register`为False，则进入这个分支，这个分支将移除所有参数化。\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparametrizations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\u001b[38;5;66;03m#    检查层是否有`parametrizations`属性。\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/utils/parametrize.py:562\u001b[0m, in \u001b[0;36mregister_parametrization\u001b[0;34m(module, tensor_name, parametrization, unsafe)\u001b[0m\n\u001b[1;32m    560\u001b[0m original \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, tensor_name)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# We create this early to check for possible errors\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m parametrizations \u001b[38;5;241m=\u001b[39m \u001b[43mParametrizationList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparametrization\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Delete the previous parameter or buffer\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(module, tensor_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/utils/parametrize.py:181\u001b[0m, in \u001b[0;36mParametrizationList.__init__\u001b[0;34m(self, modules, original, unsafe)\u001b[0m\n\u001b[1;32m    175\u001b[0m         _register_parameter_or_buffer(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, originali)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsafe:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Consistency checks:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Since f : A -> B, right_inverse : B -> A, Z and original should live in B\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Z = forward(right_inverse(original))\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Z, Tensor):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA parametrization must return a tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(Z)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/utils/parametrize.py:265\u001b[0m, in \u001b[0;36mParametrizationList.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Unpack the originals for the first parametrization\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tensor:\n\u001b[0;32m--> 265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     originals \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntensors))\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/nn/modules/module.py:1550\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1546\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1547\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1548\u001b[0m             )\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1552\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/shiym_proj/Sara/utils/SaRA/minsara/sara.py:58\u001b[0m, in \u001b[0;36mSaRAParametrization._update_weights_once\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_weights_once\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_updated:  \u001b[38;5;66;03m# 只有在未更新权重的情况下才执行\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hook_handle\u001b[38;5;241m.\u001b[39mremove()  \u001b[38;5;66;03m# 移除钩子\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/shiym_proj/Sara/utils/SaRA/minsara/sara.py:87\u001b[0m, in \u001b[0;36mSaRAParametrization.update_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         Uhr \u001b[38;5;241m=\u001b[39m Uh[: r]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(init_sara_weights\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_niter_\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:                    \n\u001b[0;32m---> 87\u001b[0m             Vr, Sr, Ur \u001b[38;5;241m=\u001b[39m \u001b[43msvd_lowrank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minit_sara_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_niter_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m             Sr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m     91\u001b[0m             Uhr \u001b[38;5;241m=\u001b[39m Ur\u001b[38;5;241m.\u001b[39mt()\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/_lowrank.py:137\u001b[0m, in \u001b[0;36msvd_lowrank\u001b[0;34m(A, q, niter, M)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtype\u001b[39m, tensor_ops))\u001b[38;5;241m.\u001b[39missubset(\n\u001b[1;32m    132\u001b[0m         (torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(tensor_ops):\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    135\u001b[0m             svd_lowrank, tensor_ops, A, q\u001b[38;5;241m=\u001b[39mq, niter\u001b[38;5;241m=\u001b[39mniter, M\u001b[38;5;241m=\u001b[39mM\n\u001b[1;32m    136\u001b[0m         )\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_svd_lowrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pissa/lib/python3.10/site-packages/torch/_lowrank.py:181\u001b[0m, in \u001b[0;36m_svd_lowrank\u001b[0;34m(A, q, niter, M)\u001b[0m\n\u001b[1;32m    179\u001b[0m     B \u001b[38;5;241m=\u001b[39m matmul(A_t, Q_c) \u001b[38;5;241m-\u001b[39m matmul(M_t, Q_c)\n\u001b[1;32m    180\u001b[0m B_t \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mtranspose(B)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m B_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m q, (B_t\u001b[38;5;241m.\u001b[39mshape, q)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m B_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m n, (B_t\u001b[38;5;241m.\u001b[39mshape, n)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m B_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m B_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], B_t\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAssertionError\u001b[0m: (torch.Size([30, 30]), 768)"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(in_features=30, out_features=30)\n",
    "# Step 1: Add sara to the model\n",
    "add_sara(model)\n",
    "\n",
    "# Step 2: Collect the parameters, pass them to the optimizer\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": list(get_sara_params(model))},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "\n",
    "# Step 3: Train the model\n",
    "# for _ in range(100):\n",
    "#     x = torch.randn(1, 30)\n",
    "#     y = model(x)\n",
    "#     loss = y.sum()\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "# ...\n",
    "\n",
    "# simulate training, update the sara parameters\n",
    "model.apply(apply_to_sara(lambda x: torch.nn.init.normal_(x.lora_A)))\n",
    "model.apply(apply_to_sara(lambda x: torch.nn.init.normal_(x.lora_B)))\n",
    "\n",
    "# Step 4: export the sara parameters\n",
    "state_dict = model.state_dict()\n",
    "sara_state_dict = {k: v for k, v in state_dict.items() if name_is_sara(k)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "539e7d19",
   "metadata": {},
   "source": [
    "## Loading and Inferencing with sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9836de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add sara to your model\n",
    "add_sara(model)\n",
    "\n",
    "# Step 2: Load the sara parameters\n",
    "_ = model.load_state_dict(sara_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Merge the sara parameters into the model\n",
    "merge_sara(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccba9d68",
   "metadata": {},
   "source": [
    "## Inferencing with multiple sara models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to avoid re-adding sara to the model when rerun the cell, remove sara first \n",
    "# remove_sara(model)\n",
    "# # Step 1: Add sara to your model\n",
    "# add_sara(model)\n",
    "\n",
    "# # Step 2: Load the sara parameters\n",
    "\n",
    "# # fake 3 sets of sara parameters\n",
    "# sara_state_dict_0 = sara_state_dict\n",
    "# sara_state_dict_1 = {k: torch.ones_like(v) for k, v in sara_state_dict.items()}\n",
    "# sara_state_dict_2 = {k: torch.zeros_like(v) for k, v in sara_state_dict.items()}\n",
    "# sara_state_dicts = [sara_state_dict_0, sara_state_dict_1, sara_state_dict_2]\n",
    "\n",
    "# load_multiple_sara(model, sara_state_dicts)\n",
    "\n",
    "# # Step 3: Select which sara to use at inference time\n",
    "# Y0 = select_sara(model, 0)(x)\n",
    "# Y1 = select_sara(model, 1)(x)\n",
    "# Y2 = select_sara(model, 2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67602a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y0, Y1, Y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_sara(model)\n",
    "# init_state_dict = model.state_dict()\n",
    "# # verify that it's the same as if we load the sara parameters one by one\n",
    "# for state_dict in sara_state_dicts:\n",
    "#     remove_sara(model)\n",
    "#     _ = model.load_state_dict(init_state_dict, strict=False)\n",
    "#     add_sara(model)\n",
    "#     _ = model.load_state_dict(state_dict, strict=False)\n",
    "#     merge_sara(model)\n",
    "#     y = model(x)\n",
    "#     print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Test():\n",
    "#     def __init__(self,num=1, layer=None):\n",
    "#         # self.layer = layer\n",
    "#         # for arg in args:\n",
    "#             # print(arg)\n",
    "#         self.layer = layer\n",
    "#         self.num = num\n",
    "# layer = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(5, 3),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(3, 3),\n",
    "#             torch.nn.ReLU()\n",
    "# )\n",
    "# test = Test(1,layer=layer)\n",
    "# # print(test)        \n",
    "# # print(test.num) # 1\n",
    "# # print(test.layer) # Linear(in_features=5, out_features=3, bias=True)\n",
    "# # inspect(test.layer)\n",
    "# print(test.layer[0])\n",
    "# print(test.layer[0].weight)\n",
    "# # print(test.layer[0]) \n",
    "# # print(test.layer[0].weight) \n",
    "# \"\"\"Parameter containing:\n",
    "# tensor([[-0.1163,  0.1544,  0.0566, -0.2275,  0.4066],\n",
    "#         [-0.0287, -0.3928,  0.2575, -0.1188, -0.0773],\n",
    "#         [-0.0870, -0.2780,  0.2427,  0.0463, -0.0287]], requires_grad=True)\"\"\"\n",
    "        \n",
    "# # print(test.layer.weight.shape) # torch.Size([3, 5])\n",
    "\n",
    "# # print(test.layer.weight.dtype) # torch.float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c847ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
