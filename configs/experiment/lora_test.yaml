# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: lora
  - override /model: lora
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["lora", "simple big autoencoder"]

seed: null

trainer:
  accelerator: gpu
  devices: [7]
  num_nodes: 1
  min_epochs: 10
  max_epochs: 300000
  check_val_every_n_epoch: 1
  # val_check_interval: 0.25
  log_every_n_steps: 1
  sync_batchnorm: True
  deterministic: False
  gradient_clip_val: 0.5
  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds

model:
  optimizer:
    _target_: torch.optim.SGD
    _partial_: true
    lr: 0.000001
    momentum: 0.9
    # lr: 0.002
  net:
    in_dim: 372320
    input_noise_factor: 0.001
    latent_noise_factor: 0.1
    fold_rate: 8
    kernel_size: 8
    enc_channel_list: [1024, 512, 512, 256, 256]
    dec_channel_list: [256, 256, 256, 128, 128]
    # enc_channel_list: [128,128,256,512]
    # dec_channel_list: [512,512,256, 8]
  compile: True

data:
  train_factor: 0.9
  val_factor: 0.05
  batch_size: 2
  num_workers: 0

logger:
  wandb:
    tags: ${tags}
    group: "lora"
    name: "liandan*10_lr=0.000001"
